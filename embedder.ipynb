{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers gdown --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 1/1 [00:13<00:00, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Примеры\n",
    "input_texts = [\n",
    "    \"Когда был спущен на воду первый миноносец «Спокойный»?\",\n",
    "    \"Есть ли нефть в Удмуртии?\",\n",
    "    \"Спокойный (эсминец)\\nЗачислен в списки ВМФ СССР 19 августа 1952 года.\",\n",
    "    \"Нефтепоисковые работы в Удмуртии были начаты сразу после Второй мировой войны в 1945 году и продолжаются по сей день. Добыча нефти началась в 1967 году.\"\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Загружаем модель и токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER-bge-m3\", use_fast=True)\n",
    "model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Включаем TF32 для всех float32 матричных умножений\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Опционально ускоряем на через compile\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "def model_encode(input_texts, batch_size = 64):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(input_texts), batch_size), desc=\"Encoding batches\"):\n",
    "        batch_texts = input_texts[i:i+batch_size]\n",
    "        \n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            token_embeddings = model_output[0]  # [batch, seq_len, hidden]\n",
    "\n",
    "            # Mean pooling и маска внимания\n",
    "            mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            summed = torch.sum(token_embeddings * mask, dim=1)\n",
    "            summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "            embeddings = summed / summed_mask\n",
    "\n",
    "            # Нормализация\n",
    "            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    # Объединяем все батчи\n",
    "    sentence_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "print(model_encode(input_texts).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10PIu9zXbrKVRfP_hGDJDgxdM7PXN6lmD\n",
      "To: /teamspace/studios/this_studio/chunks_sub.json\n",
      "100%|██████████████████████████████████████| 46.9M/46.9M [00:01<00:00, 39.3MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=17pSAjmKKcaMbMr1TnpS9Qf0NxRPzOsCy\n",
      "From (redirected): https://drive.google.com/uc?id=17pSAjmKKcaMbMr1TnpS9Qf0NxRPzOsCy&confirm=t&uuid=3e1ccc58-db8f-45cb-9825-61eb4a2f0f87\n",
      "To: /teamspace/studios/this_studio/embeddings.pt\n",
      "100%|██████████████████████████████████████| 81.9M/81.9M [00:02<00:00, 32.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Чтобы не запускать эмбеддер, можно скачать готовые эмебеддинги и чанки\n",
    "\n",
    "!gdown 10PIu9zXbrKVRfP_hGDJDgxdM7PXN6lmD  \n",
    "!gdown 17pSAjmKKcaMbMr1TnpS9Qf0NxRPzOsCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# Для теста уменьшенный возьмем датасет \n",
    "subsample = random.choices(chunks, k = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 105/105 [09:52<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "embs = model_encode(subsample, batch_size = 192) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embs, \"embeddings.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"chunks_sub.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(subsample, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
